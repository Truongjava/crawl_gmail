from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import pandas as pd
import time
import csv
import requests
from bs4 import BeautifulSoup
import re
import os
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
from itertools import product
import multiprocessing as mp
from functools import partial
import google.generativeai as genai


genai.configure(api_key="AIzaSyBK4Jep3kc--b0DDBIT8nWu2AZQIyK4as8")
# ===============================
# C·∫§U H√åNH T√åM KI·∫æM SONG SONG
# ===============================
# PLACES = [
#     'hospital',
#     'clinic',
#     'medical center',
#     'healthcare',
#     'health center',
#     'urgent care'
# ]

# PROVINCES = [
#     'Arzberg (Bavaria)',
#     'Aschaffenburg (Bavaria)',
#     'Aschersleben (Saxony-Anhalt)',
#     'Asperg (Baden-W√ºrttemberg)',
#     'A√ülar (Hesse)',
#     'Attendorn (North Rhine-Westphalia)',
#     'Aub (Bavaria)',
#     'Aue-Bad Schlema (Saxony)'
# ]


def generate_places_and_provinces(country, max_places=10, max_provinces=20):
    prompt = f"""
List exactly {max_places} common types of medical facilities (like hospitals, clinics) and exactly {max_provinces} real provinces or cities in {country}. 
Return as 2 Python lists in valid code format:
PLACES = [...]
PROVINCES = [...]
"""

    model = genai.GenerativeModel("gemini-2.0-flash")
    response = model.generate_content(prompt)

    # X·ª≠ l√Ω k·∫øt qu·∫£ tr·∫£ v·ªÅ t·ª´ Gemini
    raw = response.text.strip()

    # B·ªè markdown ``` n·∫øu c√≥
    cleaned_code = "\n".join(
        line for line in raw.splitlines() if not line.strip().startswith("```")
    )

    # Debug n·∫øu c·∫ßn
    print("üì• K·∫øt qu·∫£ t·ª´ Gemini:")
    print(cleaned_code)

    # Th·ª±c thi code ƒë·ªÉ l·∫•y 2 bi·∫øn PLACES v√† PROVINCES
    local_vars = {}
    try:
        exec(cleaned_code, {}, local_vars)
        places = local_vars.get("PLACES", [])
        provinces = local_vars.get("PROVINCES", [])

        # Ki·ªÉm tra k·∫øt qu·∫£ c√≥ ƒë√∫ng ƒë·ªãnh d·∫°ng list kh√¥ng
        if not isinstance(places, list) or not isinstance(provinces, list):
            raise ValueError("PLACES ho·∫∑c PROVINCES kh√¥ng ph·∫£i l√† danh s√°ch")

        return places, provinces

    except Exception as e:
        print("‚ùå L·ªói khi th·ª±c thi k·∫øt qu·∫£ t·ª´ Gemini:")
        print(e)
        return [], []


# C·∫•u h√¨nh song song h√≥a
MAX_WORKERS_SEARCH = 4  # S·ªë lu·ªìng cho t√¨m ki·∫øm Google Maps
MAX_WORKERS_EMAIL = 8   # S·ªë lu·ªìng cho crawl email
BATCH_SIZE = 50         # S·ªë URL x·ª≠ l√Ω trong 1 batch

# Regex v√† c·∫•u h√¨nh email
EMAIL_REGEX = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+'
TAGS_TO_EXCLUDE = ['input', 'form', 'textarea', 'button']

# Danh s√°ch c√°c extension file c·∫ßn lo·∫°i b·ªè
INVALID_EXTENSIONS = [
    '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.webp',  # ·∫¢nh
    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',  # Documents
    '.zip', '.rar', '.tar', '.gz',  # Archives
    '.mp3', '.mp4', '.avi', '.mov', '.wav',  # Media
    '.css', '.js', '.html', '.xml', '.json',  # Web files
    '.exe', '.msi', '.dmg', '.deb',  # Executables
    '.txt', '.log', '.tmp'  # Text files
]

# Danh s√°ch c√°c domain kh√¥ng h·ª£p l·ªá
INVALID_DOMAINS = [
    'example.com', 'test.com', 'localhost', 'domain.com',
    'yoursite.com', 'website.com', 'company.com', 'email.com',
    'sentry.io', 'facebook.com', 'twitter.com', 'instagram.com',
    'linkedin.com', 'youtube.com', 'google.com', 'microsoft.com',
    'sentry.wixpress.com','sentry-next.wixpress.com'
]

# C√°c pattern email kh√¥ng h·ª£p l·ªá
INVALID_PATTERNS = [
    r'.*@.*\.(png|jpg|jpeg|gif|bmp|svg|webp|pdf|doc|docx|css|js)$',
    r'.*@\d+\.\d+\.\d+\.\d+$',  # IP address
    r'^[^@]*@[^@]*@.*$',  # Nhi·ªÅu h∆°n 1 k√Ω t·ª± @
    r'.*@.*\.$',  # K·∫øt th√∫c b·∫±ng d·∫•u ch·∫•m
    r'.*@\.$',  # @ theo sau b·ªüi d·∫•u ch·∫•m
    r'.*@.*\.\w{1}$',  # Domain ch·ªâ c√≥ 1 k√Ω t·ª±
    r'.*@.*\.\w{5,}$',  # Domain extension qu√° d√†i (>4 k√Ω t·ª±)
]

# Lock cho thread-safe operations
output_lock = threading.Lock()
progress_lock = threading.Lock()

def is_valid_email(email):
    """Ki·ªÉm tra email c√≥ h·ª£p l·ªá kh√¥ng"""
    email = email.lower().strip()
    
    # Ki·ªÉm tra format c∆° b·∫£n
    if not email or '@' not in email or email.count('@') != 1:
        return False
    
    try:
        local_part, domain = email.split('@')
        
        # Ki·ªÉm tra local part
        if not local_part or len(local_part) < 1:
            return False
            
        # Ki·ªÉm tra domain
        if not domain or '.' not in domain:
            return False
            
        # Ki·ªÉm tra extension file
        if any(email.endswith(ext) for ext in INVALID_EXTENSIONS):
            return False
            
        # Ki·ªÉm tra domain kh√¥ng h·ª£p l·ªá
        if domain in INVALID_DOMAINS:
            return False
            
        # Ki·ªÉm tra pattern kh√¥ng h·ª£p l·ªá
        for pattern in INVALID_PATTERNS:
            if re.match(pattern, email):
                return False
                
        # Ki·ªÉm tra domain c√≥ √≠t nh·∫•t 1 d·∫•u ch·∫•m v√† kh√¥ng b·∫Øt ƒë·∫ßu/k·∫øt th√∫c b·∫±ng d·∫•u ch·∫•m
        if domain.startswith('.') or domain.endswith('.') or '..' in domain:
            return False
            
        # Ki·ªÉm tra domain extension h·ª£p l·ªá (2-4 k√Ω t·ª±)
        domain_parts = domain.split('.')
        if len(domain_parts) < 2:
            return False
            
        last_part = domain_parts[-1]
        if not (2 <= len(last_part) <= 4) or not last_part.isalpha():
            return False
            
        # Ki·ªÉm tra kh√¥ng ch·ª©a k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng h·ª£p l·ªá
        invalid_chars = ['<', '>', '"', "'", '\\', '/', '?', '#', '%', '&', '=']
        if any(char in email for char in invalid_chars):
            return False
            
        return True
        
    except Exception:
        return False

def driver_setup():
    """Kh·ªüi t·∫°o Edge WebDriver v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u"""
    options = webdriver.EdgeOptions()
    options.add_argument('--start-maximized')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-images')  # T·∫Øt t·∫£i ·∫£nh ƒë·ªÉ tƒÉng t·ªëc
    options.add_argument('--disable-javascript')  # T·∫Øt JS kh√¥ng c·∫ßn thi·∫øt
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument('--headless=new')  # Ch·∫°y tr√¨nh duy·ªát kh√¥ng giao di·ªán

    service = Service()
    driver = webdriver.Edge(service=service, options=options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    driver.get('https://www.google.com/maps')
    return driver

def tear_down(driver):
    """ƒê√≥ng WebDriver"""
    try:
        driver.quit()
    except:
        pass

def scroll_results(driver, pause_time=1, max_scrolls=20):
    """Cu·ªôn ƒë·ªÉ t·∫£i th√™m k·∫øt qu·∫£ tr√™n Google Maps - t·ªëi ∆∞u h√≥a"""
    scrollable_div_xpath = '//div[@role="feed"]'
    
    try:
        scrollable_div = driver.find_element(By.XPATH, scrollable_div_xpath)
        
        for i in range(max_scrolls):
            driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)
            time.sleep(pause_time)
                
    except Exception as e:
        with progress_lock:
            print(f"‚ö†Ô∏è L·ªói khi cu·ªôn: {e}")

def extract_websites_for_keyword(place, province, thread_id):
    """L·∫•y danh s√°ch website cho m·ªôt keyword c·ª• th·ªÉ"""
    keyword = f"{place} {province}"
    thread_name = f"Thread-{thread_id}"
    
    with progress_lock:
        print(f"üîç [{thread_name}] ƒêang t√¨m ki·∫øm: {keyword}")
    
    driver = None
    try:
        driver = driver_setup()
        driver.implicitly_wait(3)

        text_box = driver.find_element(By.ID, "searchboxinput")
        text_box.clear()
        text_box.send_keys(keyword)
        text_box.send_keys(Keys.ENTER)
        time.sleep(3)

        scroll_results(driver)

        # T√¨m c√°c th·∫ª ch·ª©a link website
        selectors = [
            'a.lcr4fd.S9kvJb',
            'a[data-value="Website"]',
            'a[href^="http"]:not([href*="google"])',
        ]
        
        urls = set()
        
        for selector in selectors:
            try:
                website_links = driver.find_elements(By.CSS_SELECTOR, selector)
                
                for link in website_links:
                    href = link.get_attribute('href')
                    if href and href.startswith("http") and 'google' not in href.lower():
                        urls.add(href.strip())
                        
            except Exception:
                continue

        with progress_lock:
            print(f"‚úÖ [{thread_name}] {keyword}: {len(urls)} websites")
                        
        return {
            'keyword': keyword,
            'place': place,
            'province': province,
            'urls': list(urls),
            'count': len(urls)
        }
        
    except Exception as e:
        with progress_lock:
            print(f"‚ùå [{thread_name}] L·ªói v·ªõi {keyword}: {e}")
        return {
            'keyword': keyword,
            'place': place,
            'province': province,
            'urls': [],
            'count': 0
        }
        
    finally:
        if driver:
            tear_down(driver)

def extract_all_hospital_websites():
    """L·∫•y t·∫•t c·∫£ website b·ªánh vi·ªán song song"""
    print("=" * 80)
    print("üè• B∆Ø·ªöC 1: L·∫§Y WEBSITE B·ªÜNH VI·ªÜN SONG SONG T·ª™ GOOGLE MAPS")
    print("=" * 80)
    
    # T·∫°o t·∫•t c·∫£ combinations c·ªßa place v√† province
    search_combinations = list(product(PLACES, PROVINCES))
    total_searches = len(search_combinations)
    
    print(f"üìä T·ªïng s·ªë t√¨m ki·∫øm: {total_searches}")
    print(f"üîß S·ªë lu·ªìng song song: {MAX_WORKERS_SEARCH}")
    print(f"üìç Places: {len(PLACES)} | Provinces: {len(PROVINCES)}")
    
    all_results = []
    all_urls = set()
    
    # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ ch·∫°y song song
    with ThreadPoolExecutor(max_workers=MAX_WORKERS_SEARCH) as executor:
        # Submit t·∫•t c·∫£ tasks
        future_to_combo = {
            executor.submit(extract_websites_for_keyword, place, province, i): (place, province)
            for i, (place, province) in enumerate(search_combinations, 1)
        }
        
        # Thu th·∫≠p k·∫øt qu·∫£
        for future in future_to_combo:
            try:
                result = future.result(timeout=60)  # Timeout 60s cho m·ªói t√¨m ki·∫øm
                all_results.append(result)
                all_urls.update(result['urls'])
                
            except Exception as e:
                place, province = future_to_combo[future]
                with progress_lock:
                    print(f"‚ùå Timeout/Error cho {place} {province}: {e}")

    print(f"\nüìä TH·ªêNG K√ä B∆Ø·ªöC 1:")
    print(f"   ‚Ä¢ T·ªïng s·ªë t√¨m ki·∫øm: {len(all_results)}")
    print(f"   ‚Ä¢ T·ªïng s·ªë URLs unique: {len(all_urls)}")
    
    return list(all_urls), all_results

def extract_hospital_name_from_url(url):
    """Tr√≠ch xu·∫•t t√™n b·ªánh vi·ªán t·ª´ URL ho·∫∑c n·ªôi dung website"""
    try:
        # Th·ª≠ l·∫•y t·ª´ domain name tr∆∞·ªõc
        from urllib.parse import urlparse
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()
        
        # Lo·∫°i b·ªè www. v√† subdomain
        domain_parts = domain.split('.')
        if len(domain_parts) >= 2:
            # L·∫•y ph·∫ßn ch√≠nh c·ªßa domain (b·ªè www, subdomain)
            if domain_parts[0] == 'www':
                main_domain = domain_parts[1]
            else:
                main_domain = domain_parts[0]
            
            # L√†m s·∫°ch v√† format t√™n
            hospital_name = main_domain.replace('-', ' ').replace('_', ' ')
            hospital_name = ' '.join(word.capitalize() for word in hospital_name.split())
            
            # Th√™m t·ª´ kh√≥a y t·∫ø n·∫øu ch∆∞a c√≥
            medical_keywords = ['hospital', 'medical', 'health', 'clinic', 'care']
            has_medical_keyword = any(keyword in hospital_name.lower() for keyword in medical_keywords)
            
            if not has_medical_keyword:
                hospital_name += " Medical Center"
            
            return hospital_name
    except:
        pass
    
    return "Unknown Hospital"

def extract_emails_from_url_batch(url_batch):
    """Tr√≠ch xu·∫•t email t·ª´ m·ªôt batch URLs"""
    batch_results = {}
    
    for url in url_batch:
        try:
            emails = extract_emails_from_url_single(url)
            if emails:
                batch_results[url] = emails
                
        except Exception as e:
            with progress_lock:
                print(f"‚ö†Ô∏è L·ªói v·ªõi {url}: {e}")
            
        time.sleep(0.5)  # Gi·∫£m delay gi·ªØa requests
    
    return batch_results

def extract_emails_from_url_single(url):
    """Tr√≠ch xu·∫•t email t·ª´ m·ªôt URL ƒë∆°n"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code != 200:
            return set()
            
        soup = BeautifulSoup(response.text, 'lxml')

        # Lo·∫°i b·ªè c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt
        for tag in TAGS_TO_EXCLUDE:
            for element in soup.find_all(tag):
                element.decompose()

        # T√¨m email trong text v√† HTML
        text_content = soup.get_text(separator=' ')
        html_content = str(soup)
        combined = text_content + ' ' + html_content
        
        # T√¨m t·∫•t c·∫£ email matches
        raw_emails = re.findall(EMAIL_REGEX, combined)
        
        # L·ªçc email h·ª£p l·ªá
        valid_emails = set()
        
        for email in raw_emails:
            if is_valid_email(email):
                valid_emails.add(email.lower())
        
        return valid_emails
        
    except Exception:
        return set()

def extract_emails_from_websites_parallel(urls):
    """Tr√≠ch xu·∫•t email t·ª´ danh s√°ch website song song"""
    print("\n" + "=" * 80)
    print("üìß B∆Ø·ªöC 2: TR√çCH XU·∫§T EMAIL SONG SONG T·ª™ C√ÅC WEBSITE")
    print("=" * 80)
    
    if not urls:
        print("‚ùå Kh√¥ng c√≥ URL n√†o ƒë·ªÉ crawl email!")
        return {}

    total_urls = len(urls)
    print(f"üìä T·ªïng s·ªë URLs c·∫ßn crawl: {total_urls}")
    print(f"üîß S·ªë lu·ªìng song song: {MAX_WORKERS_EMAIL}")
    print(f"üì¶ Batch size: {BATCH_SIZE}")

    # Chia URLs th√†nh batches
    url_batches = [urls[i:i + BATCH_SIZE] for i in range(0, len(urls), BATCH_SIZE)]
    
    all_results = {}
    processed_count = 0
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS_EMAIL) as executor:
        # Submit c√°c batches
        future_to_batch = {
            executor.submit(extract_emails_from_url_batch, batch): batch
            for batch in url_batches
        }
        
        for future in future_to_batch:
            try:
                batch_results = future.result(timeout=120)  # Timeout 2 ph√∫t cho m·ªói batch
                all_results.update(batch_results)
                
                batch = future_to_batch[future]
                processed_count += len(batch)
                
                with progress_lock:
                    print(f"‚úÖ Processed {processed_count}/{total_urls} URLs. Found emails in {len(batch_results)} URLs.")
                    
            except Exception as e:
                batch = future_to_batch[future]
                processed_count += len(batch)
                with progress_lock:
                    print(f"‚ùå Batch failed: {e}")

    # T√≠nh to√°n k·∫øt qu·∫£
    total_valid_emails = sum(len(emails) for emails in all_results.values())
    failed_count = total_urls - len(all_results)

    print(f"\n‚úÖ HO√ÄN TH√ÄNH B∆Ø·ªöC 2:")
    print(f"   ‚Ä¢ URLs th√†nh c√¥ng: {len(all_results)}")
    print(f"   ‚Ä¢ URLs th·∫•t b·∫°i: {failed_count}")
    print(f"   ‚Ä¢ T·ªïng emails t√¨m ƒë∆∞·ª£c: {total_valid_emails}")

    return all_results

def save_emails_to_csv_parallel(results, filename="hospital_emails_result_parallel.csv"):
    """L∆∞u k·∫øt qu·∫£ email v√†o file CSV v·ªõi ƒë·ªãnh d·∫°ng m·ªõi - thread-safe"""
    total_emails = sum(len(emails) for emails in results.values())
    print(f"\nüìä T·ªïng s·ªë emails ƒë·ªÉ l∆∞u: {total_emails}")

    if total_emails == 0:
        print(f"‚ùå Kh√¥ng c√≥ email n√†o ƒë·ªÉ l∆∞u")
        return

    with output_lock:
        with open(filename, mode='w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file)
            # Header m·ªõi theo y√™u c·∫ßu: Email, T√™n b·ªánh vi·ªán, Location
            writer.writerow(["Email", "T√™n b·ªánh vi·ªán", "Location"])

            for url, emails in results.items():
                # Tr√≠ch xu·∫•t t√™n b·ªánh vi·ªán t·ª´ URL
                hospital_name = extract_hospital_name_from_url(url)
                
                for email in sorted(emails):
                    writer.writerow([email, hospital_name])

        print(f"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: {filename}")

def display_results_parallel(results):
    """Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi c√πng v·ªõi th·ªëng k√™ chi ti·∫øt"""
    print(f"\n" + "=" * 80)
    print(f"üìä K·∫æT QU·∫¢ CU·ªêI C√ôNG - SONG SONG H√ìA")
    print("=" * 80)
    
    if not results:
        print("‚ùå KH√îNG T√åM ƒê∆Ø·ª¢C EMAIL H·ª¢P L·ªÜ N√ÄO!")
        return
    
    total_valid_emails = sum(len(emails) for emails in results.values())
    unique_emails = set()
    
    for emails in results.values():
        unique_emails.update(emails)
    
    print(f"üìä Websites c√≥ email: {len(results)}")
    print(f"üìä T·ªïng s·ªë emails: {total_valid_emails}")
    print(f"üìä Emails duy nh·∫•t: {len(unique_emails)}")
    
    # Th·ªëng k√™ theo domain
    domains = {}
    for email in unique_emails:
        if '@' in email:
            domain = email.split('@')[1]
            domains[domain] = domains.get(domain, 0) + 1
    
    if domains:
        print(f"\nüîù TOP 10 DOMAINS:")
        for domain, count in sorted(domains.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"   ‚Ä¢ {domain}: {count} emails")
    
    # Th·ªëng k√™ theo t·ªânh (n·∫øu c√≥ th·ªÉ x√°c ƒë·ªãnh t·ª´ domain)
    provinces_found = {}
    for email in unique_emails:
        if '@' in email:
            domain = email.split('@')[1].lower()
            for province in PROVINCES:
                if province.lower() in domain:
                    provinces_found[province] = provinces_found.get(province, 0) + 1
                    break
    
    if provinces_found:
        print(f"\nüó∫Ô∏è EMAILS THEO T·ªàNH (t·ª´ domain):")
        for province, count in sorted(provinces_found.items(), key=lambda x: x[1], reverse=True):
            print(f"   ‚Ä¢ {province}: {count} emails")

def main():
    """H√†m ch√≠nh ch·∫°y to√†n b·ªô quy tr√¨nh song song"""
    start_time = time.time()
    
    # Th√™m d√≤ng sau v√†o ƒë·∫ßu h√†m main()
    global PLACES, PROVINCES
    PLACES, PROVINCES = generate_places_and_provinces("Russian")

    print("=" * 80)
    print("üöÄ HOSPITAL EMAIL CRAWLER - SONG SONG H√ìA HO√ÄN TO√ÄN")
    print("=" * 80)
    print(f"üîß C·∫•u h√¨nh:")
    print(f"   ‚Ä¢ Search threads: {MAX_WORKERS_SEARCH}")
    print(f"   ‚Ä¢ Email crawl threads: {MAX_WORKERS_EMAIL}")
    print(f"   ‚Ä¢ Batch size: {BATCH_SIZE}")
    print(f"   ‚Ä¢ Places: {len(PLACES)}")
    print(f"   ‚Ä¢ Provinces: {len(PROVINCES)}")
    print(f"   ‚Ä¢ Total combinations: {len(PLACES) * len(PROVINCES)}")
    
    try:
        # B∆∞·ªõc 1: L·∫•y website b·ªánh vi·ªán song song
        urls, search_results = extract_all_hospital_websites()
        
        if not urls:
            print("‚ùå Kh√¥ng t√¨m ƒë∆∞·ª£c website n√†o. D·ª´ng ch∆∞∆°ng tr√¨nh.")
            return
        
        # B∆∞·ªõc 2: Tr√≠ch xu·∫•t email song song
        email_results = extract_emails_from_websites_parallel(urls)
        
        # B∆∞·ªõc 3: Hi·ªÉn th·ªã v√† l∆∞u k·∫øt qu·∫£ - CH·ªà LUU 1 FILE DUY NH·∫§T
        display_results_parallel(email_results)
        save_emails_to_csv_parallel(email_results)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"\nüéâ HO√ÄN TH√ÄNH!")
        print(f"‚è±Ô∏è T·ªïng th·ªùi gian: {total_time:.2f} gi√¢y ({total_time/60:.2f} ph√∫t)")
        print(f"üìÅ Ki·ªÉm tra file k·∫øt qu·∫£:")
        print(f"   ‚Ä¢ hospital_emails_result_parallel.csv - K·∫øt qu·∫£ emails v·ªõi ƒë·ªãnh d·∫°ng m·ªõi")
        
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è Ch∆∞∆°ng tr√¨nh b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng!")
    except Exception as e:
        print(f"\n‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")

if __name__ == "__main__":
    # Thi·∫øt l·∫≠p cho multiprocessing tr√™n Windows
    mp.freeze_support()
    main()